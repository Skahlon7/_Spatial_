{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyrQSQXSWyGa"
   },
   "source": [
    "**<h3>DSCI 510: Principles of Programming for Data Science </h3>**\n",
    "<h4>Spring 2023 – Assignment 9(Week 9)</h4>\n",
    "\n",
    "Instructor: <a href=\"mailto:ambite@isi.edu\">Prof. Jose Luis Ambite </a>                       \t\t\t\t                           \n",
    "Course Producers: <a href=\"mailto:svara@usc.edu\">Samuel Vara</a>, <a href=\"mailto:svparakh@usc.edu\">Sanjana Parakh</a>, <a href=\"mailto:swarnita@usc.edu\">Swarnita Venkatraman</a>, <a href=\"mailto:gursimar@usc.edu\">Gursimar Kaur</a>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qt_4uDJ5XEQZ"
   },
   "source": [
    "**Guidelines for the Assignment:**\n",
    "\n",
    "- You have to complete the assignments individually. If you are having trouble completing the assignment do let the TAs know/post on Piazza.\n",
    "\n",
    "- Please don't copy code (either partially/fully) from your classmates. Plagiarism of any form will result in a 0 and you will be reported to SJACS (https://sjacs.usc.edu/students/academic-integrity/).\n",
    "\n",
    "- You have to fill the code in this notebook and upload it for submission. While doing this, make sure all supporting files that you download from D2L are in the same directory as this notebook.  \n",
    "\n",
    "- File Naming Convention : Lab9_LastName_FirstName.ipynb\n",
    "\n",
    "- Don't change existing methods (if any, unless otherwise mentioned in the instructions)\n",
    "\n",
    "- You are encouraged to look up resources online like python docs and stackoverflow. Look up the topics and not the questions themselves. \n",
    "\n",
    "- Your last submission before the deadline will be counted towards your grade. You can submit any number of times before the deadline.\n",
    "\n",
    "- You need to take input from file/console only if it is mentioned in the question. If not we need just the logic/implementation of your function.\n",
    "\n",
    "- Scores are capped at 10 points including the Bonus problem. For Example: Even if you get a score of 12/10, your score is capped at 10.\n",
    "\n",
    "- Bonus score is counted only if the first 2 questions are attempted. Don't attempt the bonus question if you haven't attempted the first 2.\n",
    "\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP6kKpY4YOXs"
   },
   "source": [
    "### **Question 1 - BeautifulSoup Library 1[5 points]**\n",
    "\n",
    "Scrape the website - https://www.dailynews.com/ using the BeautifulSoup library. \n",
    "\n",
    "You will be taking a user input for a keyword. Out of all the headlines present on the website, match the headline texts that contain the keyword. [Case Insensitive]\n",
    "\n",
    "Return those headlines and their corresponding article link in a dictionary (the dictionary key would be the headline and its value would be the corresponding article link).\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "This is how an anchor tag retrieved from this website would look like:\n",
    "\n",
    "`<a class=\"article-title\" href=\"https://www.dailynews.com/2023/02/13/patrick-mahomes-wants-disney-to-build-more-parks-for-his-super-bowl-mvp-world-tour/\" title=\"Patrick Mahomes wants Disney to build more parks for his Super Bowl MVP ‘world tour’\">` \n",
    "\n",
    "` <span class=\"dfm-title metered\">\n",
    "\t\t\tPatrick Mahomes wants Disney to build more parks for his Super Bowl MVP ‘world tour’\t\t</span>`\n",
    "\n",
    "`</a>\n",
    "` \n",
    "\n",
    "where the headline text part is enclosed between the 'span' tags.\n",
    "\n",
    "\n",
    "1.   Retrieve all anchor tags to check for headlines and their article links. \n",
    "2.   Use strip() over the link and the headline text, so that it is more readable.\n",
    "3. If you want to understand more about HTML tag structure of the website right click on say for example a headline and click on \"Inspect\".\n",
    "4. You will get the whole html code inside that tag when you retrieve information for a particular tag. If you want some specific information from that, you would need to call functions like get and get_text to get the information directly without messing with the HTML code.\n",
    "5. You can read from the official documentation. Link: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "Some starter code has been provided for your reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment following line to install beautifulsoup if you don't have it\n",
    "# !pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "def get_news(keyword):\n",
    "    content = requests.get('https://www.dailynews.com/')\n",
    "    soup = BeautifulSoup(content.content, 'html.parser')\n",
    "    content = requests.get('https://www.dailynews.com/')\n",
    "    soup = BeautifulSoup(content.content, 'html.parser')\n",
    "    a = soup.findAll('a', {'class':'article-title' })\n",
    "    head_dic = {}\n",
    "    for index in range(len(a)):\n",
    "        head_dic[a[index].find('span').text.strip()] = a[index].get('href')\n",
    "        \n",
    "    return dict([(k,v) for k,v in head_dic.items() if keyword.lower() in k.lower()] )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The coronavirus pandemic permanently destroyed our property rights': 'https://www.dailynews.com/2023/03/17/the-coronavirus-pandemic-permanently-destroyed-our-property-rights/'}"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_news('pandemic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "id": "TnsVYt2IgUuF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'‘Operation Mountain Strong’: Patchwork of volunteers bring vital supplies to snowed-in residents': 'https://www.dailynews.com/2023/03/06/operation-mountain-strong-patchwork-of-volunteers-bring-vital-supplies-to-snowed-in-residents/',\n",
       " 'Southern California’s mountain towns remain buried under snow with more on the way': 'https://www.dailynews.com/2023/02/27/southern-californias-mountain-towns-remain-buried-under-snow-with-more-on-the-way/',\n",
       " 'Snow falls in cities, lightning strikes at beaches as historic storm pounds Southern California': 'https://www.dailynews.com/2023/02/25/grapevine-highway-14-closed-saturday-as-storm-pounds-southern-california/'}"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = 'Snow'#write your code to take user input\n",
    "get_news(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEWwdUQ7P2p4"
   },
   "outputs": [],
   "source": [
    "#do not change this cell\n",
    "\n",
    "#get_news('basketball')\n",
    "#get_news('Snow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eis6TT06sVUS"
   },
   "source": [
    "###**Question 2 - BeautifulSoup Library 2 [5 points]**\n",
    "\n",
    "Scrape the table of contents of the website - https://en.wikipedia.org/wiki/Los_Angeles using the BeautifulSoup Library. Print each entry in the table of contents on a separate line.\n",
    "\n",
    "For example output of the table of contents of the above page would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73PnqIOBtmxk"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "1Pronunciation of the name\n",
    "2History\n",
    "2.1Pre-colonial history\n",
    "2.2Spanish rule\n",
    "2.3Mexican rule\n",
    "2.41847 to present\n",
    "3Geography\n",
    ".....\n",
    "....\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQVQ8nIbuc7o"
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "\n",
    "\n",
    "1.   Look for the 'vector-toc' div tag.\n",
    "2.   Use strip() over the table of contents text, so that it is more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "hDvEgcmjs2Rc"
   },
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "\n",
    "# url = 'https://en.wikipedia.org/wiki/Los_Angeles'\n",
    "# content = requests.get(url).content\n",
    "# soup = BeautifulSoup(content,'html.parser') \n",
    "# div = soup.findAll('div')\n",
    "# for val in range(len(div)):\n",
    "#     if div[val].get('id') == 'vector-toc':\n",
    "#         #print(div[val].prettify())\n",
    "#         li = div[val].findAll('li') #individual list entries from list of contents\n",
    "#         [print(li[tag_num].div.text.strip()) for tag_num in range(1, len(li))] \n",
    "        \n",
    "        \n",
    "\n",
    "# div_tag = soup.find('div', {'class': 'vector-toc-text'})        \n",
    "#         for tag_num in range(1, len(li)):\n",
    "#             #print(tag.prettify())\n",
    "#             #print('end')\n",
    "#             print(li[tag_num].div.text.strip())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Pronunciation of the name\n",
      "2History\n",
      "2.1Pre-colonial history\n",
      "2.2Spanish rule\n",
      "2.3Mexican rule\n",
      "2.41847 to present\n",
      "3Geography\n",
      "3.1Topography\n",
      "3.2Vegetation\n",
      "3.3Geology\n",
      "3.4Cityscape\n",
      "3.4.1Overview\n",
      "3.5Climate\n",
      "3.6Environmental issues\n",
      "4Demographics\n",
      "4.1Race and ethnicity\n",
      "4.2Religion\n",
      "4.3Homelessness\n",
      "4.4Crime\n",
      "5Economy\n",
      "6Arts and culture\n",
      "6.1Movies and the performing arts\n",
      "6.2Museums and galleries\n",
      "6.3Libraries\n",
      "6.4Landmarks\n",
      "6.5Cuisine\n",
      "7Sports\n",
      "8Government\n",
      "8.1Federal and state representation\n",
      "9Education\n",
      "9.1Colleges and universities\n",
      "9.2Schools\n",
      "10Media\n",
      "11Infrastructure\n",
      "11.1Transportation\n",
      "11.1.1Freeways\n",
      "11.1.2Transit systems\n",
      "11.1.3Airports\n",
      "11.1.4Seaports\n",
      "12Notable people\n",
      "13Sister cities\n",
      "14See also\n",
      "15Explanatory notes\n",
      "16References\n",
      "17Further reading\n",
      "17.1General\n",
      "17.2Architecture and urban theory\n",
      "17.3Race relations\n",
      "17.4LGBT\n",
      "17.5Environment\n",
      "17.6Social movements\n",
      "17.7Art and literature\n",
      "18External links\n"
     ]
    }
   ],
   "source": [
    "#USE TIME LIBRARY TO CHECK FOR OPTIMIZATION\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Los_Angeles'\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content,'html.parser') \n",
    "div = soup.findAll('div', {'id' :'vector-toc'})\n",
    "\n",
    "li = div[0].findAll('li')\n",
    "\n",
    "[print(li[tag_num].div.text.strip()) for tag_num in range(1, len(li))][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA LOGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cdn.nba.com/logos/nba/1610612738/primary/L/logo.svg\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://www.nba.com/'\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content,'html.parser') \n",
    "#div = soup.findAll('div')\n",
    "\n",
    "a = soup.find_all('a', {'class': 'Anchor_anchor__cSc3P NavTeamList_ntlTeam__9K_aX'})\n",
    "\n",
    "\n",
    "img_paths = []\n",
    "for val in range(len(a)):\n",
    "    #print(a[val])\n",
    "    #print('end')\n",
    "    img_paths.append(a[val].find('img').get('src'))\n",
    "#     print()\n",
    "#     print()\n",
    "#images = soup.find_all(['img'])\n",
    "print(img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "imgs = []\n",
    "for i in range(3):\n",
    "  imgs.append(Image(url=img_paths[i]))\n",
    "\n",
    "#display(*imgs) #- view first 3 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image 1 ...\n",
      "Downloading image 2 ...\n",
      "Downloading image 3 ...\n",
      "Downloading image 4 ...\n",
      "Downloading image 5 ...\n",
      "Downloading image 6 ...\n",
      "Downloading image 7 ...\n",
      "Downloading image 8 ...\n",
      "Downloading image 9 ...\n",
      "Downloading image 10 ...\n",
      "Downloading image 11 ...\n",
      "Downloading image 12 ...\n",
      "Downloading image 13 ...\n",
      "Downloading image 14 ...\n",
      "Downloading image 15 ...\n",
      "Downloading image 16 ...\n",
      "Downloading image 17 ...\n",
      "Downloading image 18 ...\n",
      "Downloading image 19 ...\n",
      "Downloading image 20 ...\n",
      "Downloading image 21 ...\n",
      "Downloading image 22 ...\n",
      "Downloading image 23 ...\n",
      "Downloading image 24 ...\n",
      "Downloading image 25 ...\n",
      "Downloading image 26 ...\n",
      "Downloading image 27 ...\n",
      "Downloading image 28 ...\n",
      "Downloading image 29 ...\n",
      "Downloading image 30 ...\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import ssl#  --> for ssl error\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "for index, image_path in enumerate(img_paths, start=1):\n",
    "    print(f'Downloading image {index} ...')\n",
    "            \n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36')]\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "    urllib.request.urlretrieve(image_path, f'Image {index}.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNxRi1JgxoRF"
   },
   "source": [
    "### **Bonus Question - ASCII [5 points]**\n",
    "\n",
    "Use the same code that you wrote for Question 2 to get the table of contents of the French version of the page (website - https://fr.wikipedia.org/wiki/Los_Angeles). \n",
    "\n",
    "Find all the individual words from the headings in the table of contents that contain a non-ASCII character (i.e, any character with ordinal greater than 127) and store those words in a list called 'non_ascii_headings'. Print the list in sorted ascending order.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Example:\n",
    "\n",
    "For heading '9.9Personnalités liées à la ville' only the words 'Personnalités', 'liées' and 'à' must be in the final list 'non_ascii_headings'.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "\n",
    "\n",
    "1.   You will have to clean up the numbers from the headings and use .split()\n",
    "2.   Each word must appear only once in the list even if it has more than one non-ASCII character.\n",
    "3.  It is fine if '«' and '»' are a part of your final list from the heading '4.2« Recentralisation »'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "id": "C3wL4FecsNAG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cinéma',\n",
       " 'Criminalité',\n",
       " 'Diversité',\n",
       " 'Démographie',\n",
       " 'Géographie',\n",
       " 'Judaïsme',\n",
       " 'Littérature',\n",
       " 'Médias',\n",
       " 'Personnalités',\n",
       " 'Références',\n",
       " 'Réseau',\n",
       " 'Séries',\n",
       " 'aérien',\n",
       " 'dessinée',\n",
       " 'liées',\n",
       " 'références',\n",
       " 'société',\n",
       " 'télévisées',\n",
       " 'vidéo',\n",
       " '«',\n",
       " '»',\n",
       " 'Économie',\n",
       " 'à']"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write your code below\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://fr.wikipedia.org/wiki/Los_Angeles'\n",
    "\n",
    "non_ascii_headings=[]\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content,'html.parser') \n",
    "soup = BeautifulSoup(content,'html.parser') \n",
    "div = soup.findAll('div', {'id' :'vector-toc'})\n",
    "li = div[0].findAll('li')\n",
    "\n",
    "\n",
    "for val in range(1,len(li)):\n",
    "    index = 0\n",
    "    for word in li[val].div.text.strip()[0:5]:\n",
    "        if word.isnumeric() or word == '.':\n",
    "            index+=1\n",
    "    \n",
    "    words = li[val].div.text.strip()[index:].split()\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            if ord(char) > 127:\n",
    "                non_ascii_headings.append(word)\n",
    "\n",
    "            \n",
    "sorted(list(set(non_ascii_headings)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
